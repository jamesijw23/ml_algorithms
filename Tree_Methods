library(rpart)          ### CART
library(modelr)         ## Partition Data set
library(xgboost)        ## XGBoost 
library(randomForest)   ## Random Forest
library(dplyr)          ## Data Manipulation


#################################
##  Metrics Functions
#################################
metrics_function = function(matrixM){
  ##---------------------------------------
  ## Condition positive (P) - The number of real positive cases in the data
  ##---------------------------------------
  P_cond = sum(matrixM[1,])
  ##---------------------------------------
  ## Condition negative (N) - The number of real negative cases in the data
  ##---------------------------------------
  N_cond = sum(matrixM[2,])
  ##---------------------------------------
  ## True positive (TP) - Eqv. with hit
  ##---------------------------------------
  TP_cond = matrixM[1,1]
  ##---------------------------------------
  ## True negative (TN) - Eqv. with correct rejection
  ##---------------------------------------
  TN_cond = matrixM[2,2]
  ##---------------------------------------
  ## False positive (FP) - Eqv. with false alarm, Type I error
  ##---------------------------------------
  FP_cond = matrixM[1,2]
  ##---------------------------------------
  ## False negative (FN) - Eqv. with miss, Type II error
  ##---------------------------------------
  FN_cond = matrixM[2,1]
  
  
  
  ## Metrics
  ## 1) Sensitivity/Recall/True Positive Rate
  TPR_metric = TP_cond / (TP_cond + FN_cond)
  ## 3) Precision/ Positive Predictive Value
  PPV_metric = TP_cond / (TP_cond + FP_cond)
  
  ## 9) Accuracy
  ACC_metric = (TP_cond + TN_cond)/ (P_cond + N_cond)
  ## 10) F1 Score
  F1s_metric = 2 *((PPV_metric*TPR_metric) / (PPV_metric + TPR_metric))
  
  
  
  metrics = cbind(TPR_metric, PPV_metric,
                  ACC_metric, F1s_metric)
  
  colnames(metrics) = c("Recall_metric","Precision_metric",
                        "Accuracy_metric", "F1_Score_metric")
  return(metrics)
}


#################################
## Reading in Data
#################################
setwd("C:/Users/james/Google Drive/Data_Science/Data/Random DataSets")
main_df = read.csv("king.csv")



#################################
## Pre-Processing Data
#################################
## Remove ids and create new variable based on bwt

## 3 outcome classes
# df = main_df %>% 
#   select(-id) %>%
#   mutate(dis_bwt = case_when( bwt >summary(bwt)[5] ~'Above',
#                               bwt >=summary(bwt)[2]  & bwt <= summary(bwt)[5] ~'Medium',
#                               TRUE ~ 'Below')) %>%
#   select(-bwt)


## 2 outcome Classes
df = main_df %>% 
  select(-id) %>%
  mutate(dis_bwt = case_when( bwt >summary(bwt)[3] ~'Above_Median',
                              TRUE ~ 'Below')) %>%
  select(-bwt)


## Split data into train and test
p_test = 0.25
partitions = resample_partition(df,c(part0 = 1 - p_test,
                                     part1 = p_test))
train_df = data.frame(partitions$part0)
test_df = data.frame(partitions$part1)





## Split Data into X and Y where we will let Y be dis_bwt
x_train = train_df %>% select(-dis_bwt)
y_train = train_df %>% select(dis_bwt)
y_train = as.numeric(factor(y_train$dis_bwt))-1



x_test = test_df %>% select(-dis_bwt)
y_test = test_df %>% select(dis_bwt)
y_test = as.numeric(factor(y_test$dis_bwt))-1





## Change Datae into numeric for xgboost
## "low":1-Yes, 0-No    
## "firstep":1-Yes,0-No 
## "gender":1-F,0-M       
## "race":asian-1    black-2 hispanic-3    other-4    white-5     
## "smoker":1-Y,0-No
## "drinker"1-Y,0-No
## "educlv":High-2, Low-0, Middle-1 

yesno_tran_01 = function(x){
  ifelse(x =="No" | x=="N",0,1)
}
gen_tran_01 = function(x){
  ifelse(x =="M",0,1)
}

x_test_xgb = x_test %>%
  mutate(low_n = yesno_tran_01(low)) %>%
  select(-low) %>%
  mutate(firstep_n = yesno_tran_01(firstep)) %>%
  select(-firstep) %>%
  mutate(smoker_n = yesno_tran_01(smoker)) %>%
  select(-smoker) %>%
  mutate(drinker_n = yesno_tran_01(drinker)) %>%
  select(-drinker) %>% 
  mutate(gender_n = yesno_tran_01(gender)) %>%
  select(-gender) %>%
  mutate(race_n = case_when(race == "asian" ~0,
                            race == "black" ~1,
                            race == "hispanic" ~2,
                            race == "other" ~3,
                            TRUE ~ 4)) %>%
  select(-race) %>%
  mutate(educlv_n = case_when(educlv == "Low" ~0,
                              educlv == "Medium" ~1,
                            TRUE ~ 2)) %>%
  select(-educlv)%>%
  as.matrix()
  

x_train_xgb = x_train %>%
  mutate(low_n = yesno_tran_01(low)) %>%
  select(-low) %>%
  mutate(firstep_n = yesno_tran_01(firstep)) %>%
  select(-firstep) %>%
  mutate(smoker_n = yesno_tran_01(smoker)) %>%
  select(-smoker) %>%
  mutate(drinker_n = yesno_tran_01(drinker)) %>%
  select(-drinker) %>% 
  mutate(gender_n = yesno_tran_01(gender)) %>%
  select(-gender) %>%
  mutate(race_n = case_when(race == "asian" ~0,
                            race == "black" ~1,
                            race == "hispanic" ~2,
                            race == "other" ~3,
                            TRUE ~ 4)) %>%
  select(-race) %>%
  mutate(educlv_n = case_when(educlv == "Low" ~0,
                              educlv == "Medium" ~1,
                              TRUE ~ 2)) %>%
  select(-educlv) %>%
  as.matrix()


#################################
## ML1: CART
#################################
## Run Model
cart_model = rpart(dis_bwt ~. ,
                   method="class",
                   data = train_df
                   )
## Predict for CART
result_cart_test = predict(cart_model,data.frame(x_test))
est_cart = apply(result_cart_test,1,which.max) - 1

## Confusion matrix for CART
m_cart = table(y_test,est_cart)
m_cart = data.frame(method = "CART",
                    metrics_function(m_cart))




## Visualize Tree





#################################
## ML2: Random Forest
#################################
## Run Model
rf_model = randomForest(as.factor(dis_bwt) ~.,
                        data=train_df, 
                        importance=TRUE, 
                        ntree=2000)
## Predict for logistic regression
est_rf = predict(rf_model,data.frame(x_test))
est_rf = as.numeric(factor(est_rf))-1
## Confusion matrix for Random Forest
m_rf = table(y_test,est_rf)
m_rf = data.frame(method = "Random_Forest",
                  metrics_function(m_rf))




#################################
## ML3: XGBoost
#################################
## Run Model
xgb_model <- xgboost(data = x_train_xgb,
                     label = y_train,
                     nrounds = 10)
## Predict for logistic regression
est_xgb = ifelse(predict(xgb_model,x_test_xgb)>0.5,1,0)
## Confusion matrix for logistic regression
m_xgb = table(y_test,est_xgb)
m_xgb = data.frame(method = "XGBoost",
                   metrics_function(m_xgb))




rbind(m_cart,m_rf,m_xgb)




