library(rpart)          ### CART
library(modelr)         ## Partition Data set
library(xgboost)        ## XGBoost 
library(randomForest)   ## Random Forest
library(dplyr)          ## Data Manipulation


#################################
## Reading in Data
#################################
setwd("C:/Users/james/Google Drive/Data_Science/Data/Random DataSets")
main_df = read.csv("king.csv")



#################################
## Pre-Processing Data
#################################
## Remove ids and create new variable based on bwt

## 3 outcome classes
# df = main_df %>% 
#   select(-id) %>%
#   mutate(dis_bwt = case_when( bwt >summary(bwt)[5] ~'Above',
#                               bwt >=summary(bwt)[2]  & bwt <= summary(bwt)[5] ~'Medium',
#                               TRUE ~ 'Below')) %>%
#   select(-bwt)


## 2 outcome Classes
df = main_df %>% 
  select(-id) %>%
  mutate(dis_bwt = case_when( bwt >summary(bwt)[3] ~'Above_Median',
                              TRUE ~ 'Below')) %>%
  select(-bwt)


## Split data into train and test
p_test = 0.25
partitions = resample_partition(df,c(part0 = 1 - p_test,
                                     part1 = p_test))
train_df = data.frame(partitions$part0)
test_df = data.frame(partitions$part1)





## Split Data into X and Y where we will let Y be dis_bwt
x_train = train_df %>% select(-dis_bwt)
y_train = train_df %>% select(dis_bwt)
y_train = as.numeric(factor(y_train$dis_bwt))-1



x_test = test_df %>% select(-dis_bwt)
y_test = test_df %>% select(dis_bwt)
y_test = as.numeric(factor(y_test$dis_bwt))-1


x_test


## Work
# "low":1-Yes,0     
# "firstep":1-Yes,0 
# "gender":1-F,0       
# "race":asian-1    black-2 hispanic-3    other-4    white-5     
# "smoker":1-Y,0
# "drinker"1-Y,0
# "educlv":High-2, Low-0, Middle-1   

#################################
## ML1: CART
#################################
## Run Model
cart_model = rpart(dis_bwt ~. ,
                   method="class",
                   data = train_df
                   )
## Predict for CART
result_cart_test = predict(cart_model,data.frame(x_test))
est_cart = apply(result_cart_test,1,which.max) - 1

## Confusion matrix for CART
m_cart = table(y_test,est_cart)
m_cart = data.frame(method = "CART",
                    metrics_function(m_cart))




## Visualize Tree





#################################
## ML2: Random Forest
#################################
## Run Model
rf_model = randomForest(as.factor(dis_bwt) ~.,
                        data=train_df, 
                        importance=TRUE, 
                        ntree=2000)
## Predict for logistic regression
est_rf = predict(rf_model,data.frame(x_test))
est_rf = as.numeric(factor(est_rf))-1
## Confusion matrix for Random Forest
m_rf = table(y_test,est_rf)
m_rf = data.frame(method = "Random_Forest",
                  metrics_function(m_rf))




#################################
## ML3: XGBoost
#################################
## Run Model
xgb_model <- xgboost(data = x_train, label = y_train, nrounds = 10)
## Predict for logistic regression
est_xgb = ifelse(predict(xgb_model,x_test)>0.5,1,0)
## Confusion matrix for logistic regression
m_xgb = table(y_test,est_xgb)
m_xgb = data.frame(method = "XGBoost",
                   metrics_function(m_xgb))













#################################
##  Metrics Functions
#################################
metrics_function = function(matrixM){
  ##---------------------------------------
  ## Condition positive (P) - The number of real positive cases in the data
  ##---------------------------------------
  P_cond = sum(matrixM[1,])
  ##---------------------------------------
  ## Condition negative (N) - The number of real negative cases in the data
  ##---------------------------------------
  N_cond = sum(matrixM[2,])
  ##---------------------------------------
  ## True positive (TP) - Eqv. with hit
  ##---------------------------------------
  TP_cond = matrixM[1,1]
  ##---------------------------------------
  ## True negative (TN) - Eqv. with correct rejection
  ##---------------------------------------
  TN_cond = matrixM[2,2]
  ##---------------------------------------
  ## False positive (FP) - Eqv. with false alarm, Type I error
  ##---------------------------------------
  FP_cond = matrixM[1,2]
  ##---------------------------------------
  ## False negative (FN) - Eqv. with miss, Type II error
  ##---------------------------------------
  FN_cond = matrixM[2,1]
  
  
  
  ## Metrics
  ## 1) Sensitivity/Recall/True Positive Rate
  TPR_metric = TP_cond / (TP_cond + FN_cond)
  ## 3) Precision/ Positive Predictive Value
  PPV_metric = TP_cond / (TP_cond + FP_cond)

  ## 9) Accuracy
  ACC_metric = (TP_cond + TN_cond)/ (P_cond + N_cond)
  ## 10) F1 Score
  F1s_metric = 2 *((PPV_metric*TPR_metric) / (PPV_metric + TPR_metric))


  
  metrics = cbind(TPR_metric, PPV_metric,
                  ACC_metric, F1s_metric)
  
  colnames(metrics) = c("Recall_metric","Precision_metric",
                        "Accuracy_metric", "F1_Score_metric")
  return(metrics)
}



